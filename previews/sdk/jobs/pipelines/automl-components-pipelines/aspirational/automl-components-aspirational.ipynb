{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from azure.ml import MLClient\n",
    "import mlflow\n",
    "from azure.ml.entities import CommandJob, Code, PipelineJob, Dataset, InputDatasetEntry\n",
    "from azure.ml.entities import AutoMLJob, AutoFeaturizationJob, TestSetJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter details of your AML workspace\n",
    "subscription_id = '15ae9cb6-95c1-483d-a0e3-b1a1a3b06324'\n",
    "resource_group = 'automldpv2priprev-resgrp'\n",
    "workspace = 'automldpv2priprev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a handle to the workspace\n",
    "ml_client = MLClient(subscription_id, resource_group, workspace)\n",
    "assert client is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize MLFlow Client\n",
    "\n",
    "The models and artifacts that are produced by AutoML can be accessed by the MLFlow interface. Initialize the MLFlow client here, and set the backend as Azure ML, via. the MLFlow Client.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "Q: Can we set this (the tracking URI) inside AutoML, given things won't work at all w/o setting MLFlow context above?\n",
    "\n",
    "Q: Do we need MLFlow client for job submissions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can this MLFlow initialization call be made transparently from MLFlow Client instead of explecetely?\n",
    "\n",
    "tracking_uri = \"TODO --> Get this from MLClient\"\n",
    "\n",
    "################################################################################\n",
    "# TODO: The API to get tracking URI is not yet available on Worksapce object.\n",
    "from azureml.core import Workspace as WorkspaceV1\n",
    "ws = WorkspaceV1(workspace_name=workspace_name, resource_group=resource_group_name, subscription_id=subscription_id)\n",
    "tracking_uri = ws.get_mlflow_tracking_uri()\n",
    "del ws\n",
    "################################################################################\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(\"\\nCurrent tracking uri: {}\".format(mlflow.get_tracking_uri()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the inputs and outputs required for the pipeline\n",
    "my_local_single_dataset = Dataset(\n",
    "    local_path=\"./data\"\n",
    ")\n",
    "pipeline_job_inputs = {'raw_dataset_from_pipeline': InputDatasetEntry(dataset=my_local_single_dataset),\n",
    "                       'n_cross_validations_from_pipeline': '5',\n",
    "                       'task_from_pipeline': 'regression',\n",
    "                       'optimization_metric_from_pipeline': 'normalized_root_mean_squared_error',\n",
    "                       'target_column_name_from_pipeline': 'price'\n",
    "                      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "Q: Should parameters number based be specified as numbers, not as text? i.e. '5'. In original example from pipelines it was:\n",
    "pipeline_job_inputs = {'max_epocs_from_pipeline': '20', ...\n",
    "\n",
    "Q: Should we define pipeline outputs here or at the end?..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoFeaturization Job/Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AutoFeaturizationJob (Component?)\n",
    "\n",
    "autofeaturization_job_inputs = {'data': '${{inputs.raw_dataset_from_pipeline}}',  #upload the local data into a dataset\n",
    "                                'n_cross_validations': '${{inputs.n_cross_validations_from_pipeline}}', #take this input from the pipeline inputs\n",
    "                                'task': '${{inputs.task_from_pipeline}}', #take this input from the pipeline inputs \n",
    "                                'optimization_metric': '${{inputs.optimization_metric_from_pipeline}}', #take this input from the pipeline inputs \n",
    "                                'target_column_name': '${{inputs.target_column_name_from_pipeline}}' #take this input from the pipeline inputs  \n",
    "                               } \n",
    "                                \n",
    "autofeaturization_job_outputs = {'featurized_training_data': None,\n",
    "                                 'featurized_validation_data': None,\n",
    "                                 'featurized_test_data': None,\n",
    "                                 'featurizer': None\n",
    "                                }\n",
    "\n",
    "autofeaturization_job = AutoFeaturizationJob(\n",
    "    inputs = autofeaturization_job_inputs, #inputs to the job\n",
    "    outputs = autofeaturization_job_outputs, #outputs of the job\n",
    "    compute = 'azureml:cpu_cluster' #<override with some other compute if needed>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "\n",
    "'featurized_training_data' and 'featurized_validation_data' would contain a single dataset if training/validation split, or would contain multiple datasets each (correlated), one per CV fold. I.e. 5 correlated datasets each (implementation TBD, folders, list of objects?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues / Problems to be solved\n",
    "\n",
    "Q: What \"type\" are 'featurized_training_data' and 'featurized_validation_data'?\n",
    "   - List(TabularDataset)?\n",
    "   - Path to a blob with \"MLDatasetArtifact\" (currently in spec only, Daniel-Sch) at rest with multiple folders one per dataset?\n",
    "   - Other? \n",
    "\n",
    "\n",
    "Q: Do we need \"environment\" parameter for the AutoML-components? Or we use it internally only?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoTrain Job/Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AutoTrainJob to run in the pipeline\n",
    "\n",
    "### train_cmd = \"python train.py --training_data ${{inputs.training_data}} --test_data ${{outputs.test_data}} --model_output ${{outputs.model_output}}\"\n",
    "\n",
    "autotrain_job_inputs = {'data': '${{jobs.autofeaturization-job.outputs.featurized_training_data}}',\n",
    "                        'validation_data':  '${{jobs.autofeaturization-job.outputs.featurized_validation_data}}', \n",
    "                        'task': '${{inputs.task_from_pipeline}}', #take this input from the pipeline inputs \n",
    "                        'optimization_metric': '${{inputs.optimization_metric_from_pipeline}}', #take this input from the pipeline inputs \n",
    "                        'target_column_name': '${{inputs.target_column_name_from_pipeline}}' #take this input from the pipeline inputs\n",
    "                       } \n",
    "\n",
    "autotrain_job_outputs = {'best_model': None, 'validation_predictions': None}\n",
    "\n",
    "autotrain_job = AutoTrainJob(\n",
    "    inputs = autotrain_job_inputs, #inputs to the job\n",
    "    outputs = autotrain_job_outputs, #outputs of the job\n",
    "    compute = 'azureml:cpu_cluster' #<override with some other compute if needed>\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set Job/Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Test Set Job or component to run in the pipeline\n",
    "\n",
    "test_set_job_inputs = {'train_data': '${{jobs.autofeaturization-job.outputs.featurized_training_data}}',\n",
    "                       'test_data':  '${{jobs.autofeaturization-job.outputs.featurized_test_data}}', \n",
    "                       'task': '${{inputs.task_from_pipeline}}', #take this input from the pipeline inputs \n",
    "                       'target_column_name': '${{inputs.target_column_name_from_pipeline}}', #take this input from the pipeline inputs\n",
    "                       'model_uri': '${{jobs.autotrain-job.outputs.best_model}}'\n",
    "                       } \n",
    "\n",
    "test_set_job = TestSetJob(\n",
    "    inputs = test_set_job_inputs, #inputs to the job\n",
    "    compute = 'azureml:cpu_cluster' #<override with some other compute if needed>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the pipeline\n",
    "\n",
    "pipeline_job = PipelineJob(\n",
    "    description = 'automl-componentization-example',\n",
    "    jobs= {\n",
    "        'autofeaturization-job':autofeaturization_job, \n",
    "        'autotrain-job': autotrain_job,\n",
    "        'test-set-job': test_set_job}, #add all the jobs into this pipeline\n",
    "    inputs= pipeline_job_inputs, #top level inputs to the pipeline\n",
    "    outputs=autotrain_job_outputs,\n",
    "    compute = \"cpu-cluster\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTIONS on Pipeline Outputs\n",
    "\n",
    "Q: Is the pipeline outputs right? How can we merge multiple outputs from multiple jobs/components instead of having just the outputs from the autotrain job?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "#get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER DEVELOPMENT EXPERIENCE CONSIDERATIONS\n",
    "\n",
    "**Better way to define INPUTS/OUTPUTS?**: The definition of inputs/outputs is defined with JSON... that's a evry lose way of doing it, not intellisense, etc. prone to errors as compared to object's properties. Is there any other better way for defining the inputs/outputs instead of using JSON?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45ee23ad53d8447c1a4a7f9f605254595f8ee53c2e1723e7948bbd485e96ca91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
