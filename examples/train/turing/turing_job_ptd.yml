# yaml-language-server: $schema=https://azuremlsdk2.blob.core.windows.net/latest/commandJob.schema.json
code: 
  directory: .
command:  python -m torch.distributed.launch --nproc_per_node=8 --nnodes=2  --master_addr $AZ_BATCHAI_MPI_MASTER_NODE --master_port 11234 --node_rank $OMPI_COMM_WORLD_RANK run_seq2seq.py  --train_file {inputs.training_data}/cnndm.train.uncased_tokenized.json --output_dir /tmp/output    --model_type unilm --model_name_or_path unilm2-base-uncased --do_lower_case --fp16 --fp16_opt_level O2  --max_source_seq_length 256 --max_target_seq_length 160 --per_gpu_train_batch_size 4 --gradient_accumulation_steps 2 --learning_rate 7e-5 --num_warmup_steps 1000 --num_training_steps 2500 --cache_dir /tmp/cache --save_steps 1500 --target_mask_prob 0.7
environment: azureml:turingv3:6
experiment_name: turing-cnndm-2-node-ptd
inputs:
  training_data:
    data: azureml:cnndm-train-dir:1
    mode: mount 
distribution:
  type: mpi
  process_count_per_instance: 1    
compute:
  target: azureml:nd40v2
  instance_count: 2